# GeeCache

一个分布式缓存系统的 Go 实现，参考[7天用Go从零实现分布式缓存GeeCache](https://geektutu.com/post/geecache.html)。

## 需求分析

首先，我们的目标是一个缓存系统，即 Cache，那么就需要考虑以下因素：

- 写入策略
- 查找策略
- 替换策略

而对于分布式系统，我们需要考虑这些因素：

- 不同系统之间的一致性
- 不同系统之间的通信

思考上面这些问题后，我们开始尝试实现它。

## LRU 缓存淘汰机制

常见的缓存淘汰算法有以下几种：

- FIFO：先进先淘汰。效果并不好，它优先照顾后面加入缓存的记录，某些情况下，如果某些较早的记录被频繁访问，就可能会频繁淘汰然后又写入。
- LFU：最少使用淘汰。记录每条记录的访问次数，对记录按访问次数排序，每次淘汰时淘汰访问次数最少的记录。实现消耗略高，因为需要频繁排序(当然，局部变更插入排序即可)。此外，它优先照顾整个历史中被频繁访问的记录，但是可能某些记录在一段时间后就不再使用，而又有可能因为访问次数过高而无法淘汰。
- LRU：最近最少使用淘汰。相对平衡上面两个算法，即照顾后加入缓存的记录，也考虑访问频繁的记录。

### LRU 实现

![implement lru algorithm with golang](./img/lru.jpg)

如上图所示，我们的 LRU 通过一个 字典(map) 和一个 双向链表(list)实现。

当某个记录被访问时，就把它从双向链表中移动到链表头，即最近使用。那么链表尾的记录就是最近最少使用的记录了。我们淘汰时，淘汰队尾即可。

实现接口如下：

- New：创建一个 LRUCache 实例
- Set: 写入 Cache
- Get: 在 Cache 中查询
- Remove: 在 Cache 中删除
- RemoveOldest: 淘汰最近最少使用的记录，无需手动调用

## 单机并发缓存

我们上面的 LRU 实现了一个独立的 cache，通过 LRU 算法淘汰记录。

现在我们考虑和用户有交互的情况：

- 多个用户同时读/写：并发控制，锁
- 用户获取数据的封装：从Test的体验就感受的到，单纯定义一个接口作为 Value 使用起来并不方便
- cache 不命中时，怎么获取数据：cpu cache 是向下层的内存请求调入数据块那样，参考这种思路，我们把不命中时获取数据的动作封装在回调函数中，用户应该实现相应的接口
- 拓展：为后面的分布式系统做一些兼容准备



























